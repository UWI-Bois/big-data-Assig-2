{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID : 816000325\n",
    "## Name: Ajay Sieunarine\n",
    "## Email: ajay.sieunarine@my.uwi.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert the following sentence to a python tuple list of letters and their frequency as they appear. Ignore all non-alpha numeric characters.\n",
    "\n",
    "Sentence: \n",
    "\n",
    "“The quick brown fox jumps over the lazy dog and the fox was very happy”\n",
    "\n",
    "Tuple List:\n",
    "\n",
    "[('h', 1), ('e', 1), ('t', 1), ('q', 1), ('i', 1), ('c', 1), ('u', 1), ('k', 1), ('r', 1), ('b', 1), ('o', 1), ('w', 1), ('n', 1), ('x', 1), ('o', 1), ('f', 1), ('u', 1), ('s', 1), ('j', 1), ('m', 1), ('p', 1), ('r', 1), ('e', 1), ('o', 1), ('v', 1), ('h', 1), ('e', 1), ('t', 1), ('a', 1), ('y', 1), ('z', 1), ('l', 1), ('o', 1), ('d', 1), ('g', 1), ('a', 1), ('d', 1), ('n', 1), ('h', 1), ('e', 1), ('t', 1), ('x', 1), ('o', 1), ('f',1), ('a', 1), ('s', 1), ('w', 1), ('y', 1), ('r', 1), ('e', 1), ('v', 1), ('a', 1), ('h', 1), ('y', 1), ('p', 2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('t', 1), ('h', 1), ('e', 1), ('q', 1), ('u', 1), ('i', 1), ('c', 1), ('k', 1), ('b', 1), ('r', 1), ('o', 1), ('w', 1), ('n', 1), ('f', 1), ('o', 1), ('x', 1), ('j', 1), ('u', 1), ('m', 1), ('p', 1), ('s', 1), ('o', 1), ('v', 1), ('e', 1), ('r', 1), ('t', 1), ('h', 1), ('e', 1), ('l', 1), ('a', 1), ('z', 1), ('y', 1), ('d', 1), ('o', 1), ('g', 1), ('a', 1), ('n', 1), ('d', 1), ('t', 1), ('h', 1), ('e', 1), ('f', 1), ('o', 1), ('x', 1), ('w', 1), ('a', 1), ('s', 1), ('v', 1), ('e', 1), ('r', 1), ('y', 1), ('h', 1), ('a', 1), ('p', 2), ('y', 1)]\n",
      "<class 'list'>\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "foxy_sentence = \"The quick brown fox jumps over the lazy dog and the fox was very happy\"\n",
    "new_sentence = \"\" # empty string which will represent the sentence with all common and letters only (with spaces)\n",
    "\n",
    "for i in foxy_sentence: # populate new sentence\n",
    "    if i.isalpha() or i == \" \":\n",
    "        new_sentence += i.lower()\n",
    "\n",
    "# print (new_sentence)\n",
    "words = new_sentence.split() # words will be an array of strings representing all the words in the new_sentence\n",
    "# print(words)\n",
    "\n",
    "foxy_tuple = [] # the tuple to hold the date\n",
    "\n",
    "# foreach word, grab the frequency of each letter, \n",
    "# and store it in a new tuple, which will then be appended to the actual tuple\n",
    "for word in words: \n",
    "    freqs = {}\n",
    "    for ch in word: # \n",
    "        if ch in freqs:\n",
    "            freqs[ch] += 1\n",
    "        else:\n",
    "            freqs[ch] = 1\n",
    "    temp_tuple = tuple(freqs.items())\n",
    "    foxy_tuple += temp_tuple\n",
    "\n",
    "print(foxy_tuple)\n",
    "print(type(foxy_tuple))\n",
    "print(len(foxy_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a PySpark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported pyspark\n",
      "imported all pyspark libs\n",
      "initialized spark session\n",
      "<SparkContext master=local appName=Assignment 2>\n"
     ]
    }
   ],
   "source": [
    "import pyspark # library for the spark session -> spark will allow us to manipualte RDDs\n",
    "print (\"imported pyspark\")\n",
    "from pyspark.sql import SparkSession # spark session -> allow for dataframe and sql functionality\n",
    "from pyspark.sql import Row # allow us to represent rows from dataframes\n",
    "from pyspark.sql.types import * # get/set datatypes of cols\n",
    "from pyspark.sql.functions import * # epic built-in functions \n",
    "from pyspark.ml.linalg import DenseVector # for linear algebra, uses numpy arrays,\n",
    "from pyspark.ml.feature import StandardScaler # standardize features by removing the mean, and scaling to unit variance\n",
    "print(\"imported all pyspark libs\")\n",
    "\n",
    "# init spark sesh\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Assignment 2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print (\"initialized spark session\")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert the list of tuples to a PySpark RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('t', 1), ('h', 1), ('e', 1), ('q', 1), ('u', 1), ('i', 1), ('c', 1), ('k', 1), ('b', 1), ('r', 1), ('o', 1), ('w', 1), ('n', 1), ('f', 1), ('o', 1), ('x', 1), ('j', 1), ('u', 1), ('m', 1), ('p', 1), ('s', 1), ('o', 1), ('v', 1), ('e', 1), ('r', 1), ('t', 1), ('h', 1), ('e', 1), ('l', 1), ('a', 1), ('z', 1), ('y', 1), ('d', 1), ('o', 1), ('g', 1), ('a', 1), ('n', 1), ('d', 1), ('t', 1), ('h', 1), ('e', 1), ('f', 1), ('o', 1), ('x', 1), ('w', 1), ('a', 1), ('s', 1), ('v', 1), ('e', 1), ('r', 1), ('y', 1), ('h', 1), ('a', 1), ('p', 2), ('y', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(foxy_tuple) # make rdd from tuple\n",
    "type(rdd)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using the methods of PySpark RDD display the letter count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 4), ('b', 1), ('c', 1), ('d', 2), ('e', 5), ('f', 2), ('g', 1), ('h', 4), ('i', 1), ('j', 1), ('k', 1), ('l', 1), ('m', 1), ('n', 2), ('o', 5), ('p', 3), ('q', 1), ('r', 3), ('s', 2), ('t', 3), ('u', 2), ('v', 2), ('w', 2), ('x', 2), ('y', 3), ('z', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd_grouped = rdd.reduceByKey(lambda x, y: x+y)\n",
    "rdd_grouped = rdd_grouped.sortByKey()\n",
    "print(rdd_grouped.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using the methods of PySpark RDD display the letter and number of times they appear in each word in the sentence.\n",
    "\n",
    "Sample:\n",
    "\n",
    "[('a', [1, 1, 1, 1]), ('e', [1, 1, 1, 1, 1]), ('i', [1]), ('m', [1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "[('a', [1, 1, 1, 1]), ('b', [1]), ('c', [1]), ('d', [1, 1]), ('e', [1, 1, 1, 1, 1]), ('f', [1, 1]), ('g', [1]), ('h', [1, 1, 1, 1]), ('i', [1]), ('j', [1]), ('k', [1]), ('l', [1]), ('m', [1]), ('n', [1, 1]), ('o', [1, 1, 1, 1, 1]), ('p', [1, 2]), ('q', [1]), ('r', [1, 1, 1]), ('s', [1, 1]), ('t', [1, 1, 1]), ('u', [1, 1]), ('v', [1, 1]), ('w', [1, 1]), ('x', [1, 1]), ('y', [1, 1, 1]), ('z', [1])]\n"
     ]
    }
   ],
   "source": [
    "rdd_group_by_key = rdd.groupByKey() # make each entry in the rdd iterable\n",
    "print(type(rdd))\n",
    "    \n",
    "rdd_word_occurences = rdd_group_by_key.map(\n",
    "    lambda x: (\n",
    "        x[0], # the letter\n",
    "        list(x[1]) # the occurences as a list\n",
    "    )\n",
    ")\n",
    "rdd_word_occurences = rdd_word_occurences.sortByKey() # sort in alphabetical order\n",
    "print(rdd_word_occurences.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are a frequent user of Amazon.com, you are probably familiar with the lists of related products (books, CDs, etc.) the site features to help customers find what they are looking for. Amazon.com presents several such lists on every page, including “Frequently Bought Together” and “Customers Who Bought This Item Also Bought.” These features have roots and solutions in recommendation engines and systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine:\n",
    "•\tCustomers who bought this item also bought  \n",
    "•\tMost popular items \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tCreate a sql context from PySpark SQLContext.  \t \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x0000026F2BFD1448>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(sc)\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tLoad the Amazon Review Dataset into a PySpark RDD, ensure that each row is properly separated and the headers are matched to their respective columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_reviews.csv MapPartitionsRDD[653] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile('amazon_reviews.csv')\n",
    "rdd.collect()\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',id,name,username', '0,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Cristina M', '1,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Ricky', '2,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Tedd Gardiner', '3,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Dougal']\n"
     ]
    }
   ],
   "source": [
    "print(rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda line: line.split(\",\")) # split by delim=','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'id', 'name', 'username'],\n",
       " ['0', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Cristina M'],\n",
       " ['1', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Ricky'],\n",
       " ['2', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Tedd Gardiner'],\n",
       " ['3', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Dougal']]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5) # if getting split not in list error, run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tConvert the rdd to a PySpark DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping rdd into df...\n",
      "done.\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"mapping rdd into df...\")\n",
    "df = rdd.map(lambda line: Row(\n",
    "    key=line[0],\n",
    "    itemID=line[1],\n",
    "    itemName=line[2],\n",
    "    username=line[3],\n",
    ")).toDF()\n",
    "df = df[['key', 'itemID', 'itemName', 'username']]\n",
    "# df.drop([0,1,2])\n",
    "df = df.where(\"itemName!='name'\") # remove header row\n",
    "print(\"done.\")\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------------+-------------+\n",
      "|key|              itemID|         itemName|     username|\n",
      "+---+--------------------+-----------------+-------------+\n",
      "|  0|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|   Cristina M|\n",
      "|  1|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|        Ricky|\n",
      "|  2|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|Tedd Gardiner|\n",
      "+---+--------------------+-----------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- itemID: string (nullable = true)\n",
      " |-- itemName: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tUsing the dataframe from the above question show the top 20 bought products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+-----+\n",
      "|            itemName|count|\n",
      "+--------------------+-----+\n",
      "|Amazon Tap - Alex...|  542|\n",
      "|      Amazon Fire TV|  166|\n",
      "|Amazon Premium He...|  133|\n",
      "|    Fire HD 6 Tablet|   87|\n",
      "|\"Kindle Fire HDX ...|   53|\n",
      "|\"Kindle Fire HDX ...|   43|\n",
      "|\"Kindle Fire HD 7\"\"\"|   41|\n",
      "|   Kindle Paperwhite|   39|\n",
      "|Certified Refurbi...|   38|\n",
      "|     Kindle Keyboard|   32|\n",
      "|All-New Amazon Fi...|   27|\n",
      "|              Kindle|   20|\n",
      "|Amazon 5W USB Off...|   19|\n",
      "|All-New Amazon Fi...|   18|\n",
      "|Replacement Remot...|   17|\n",
      "|Echo Dot (2nd Gen...|   13|\n",
      "|All-New Amazon Ki...|   13|\n",
      "|Moshi Anti-Glare ...|   12|\n",
      "|All-New Amazon Ki...|   12|\n",
      "|Alexa Voice Remot...|   12|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mostBought = df.groupBy(\"itemName\").count()\n",
    "print(type(mostBought))\n",
    "mostBought.sort(\"count\", ascending=False).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tUsing the datafrme from the previous question show the top 20 users and the products that they purchased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+\n",
      "|           username|            itemName|count|\n",
      "+-------------------+--------------------+-----+\n",
      "|          A. Younan|Amazon Premium He...|   59|\n",
      "|             Andrew|Amazon Premium He...|   43|\n",
      "|          Victor L.|Amazon Premium He...|   30|\n",
      "|     William Hardin|    Fire HD 6 Tablet|   30|\n",
      "|            Mike W.|    Fire HD 6 Tablet|   29|\n",
      "|      Earthling1984|    Fire HD 6 Tablet|   28|\n",
      "|     William Hardin|      Amazon Fire TV|   16|\n",
      "|         B. Tarbuck|\"Kindle Fire HDX ...|   16|\n",
      "|              Mandy|      Amazon Fire TV|   16|\n",
      "|                 NF|\"Kindle Fire HDX ...|   15|\n",
      "|                 NF|\"Kindle Fire HD 7\"\"\"|   14|\n",
      "|    Amazon Reviewer|\"Kindle Fire HDX ...|   14|\n",
      "|    Amazon Reviewer|\"Kindle Fire HDX ...|   13|\n",
      "|             Dallas|      Amazon Fire TV|   12|\n",
      "|     William Hardin|Certified Refurbi...|   12|\n",
      "|  Michael Gallagher|\"Kindle Fire HDX ...|   12|\n",
      "|   Gregory P. Baker|      Amazon Fire TV|   11|\n",
      "|              MarkM|\"Kindle Fire HD 7\"\"\"|   10|\n",
      "|              Mandy|Certified Refurbi...|   10|\n",
      "|\"Things I Love Like|\"Kindle Fire HDX ...|    9|\n",
      "+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestCustomers = df.groupBy(\"username\", \"itemName\").count()\n",
    "bestCustomers = bestCustomers.sort(\"count\", ascending=False)\n",
    "bestCustomers.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tCreate a RDD of tuples from the dataframe with only 2 columns ‘product’ and ‘username’ in that order.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|          product|          username|\n",
      "+-----------------+------------------+\n",
      "|Kindle Paperwhite|        Cristina M|\n",
      "|Kindle Paperwhite|             Ricky|\n",
      "|Kindle Paperwhite|     Tedd Gardiner|\n",
      "|Kindle Paperwhite|            Dougal|\n",
      "|Kindle Paperwhite|Miljan David Tanic|\n",
      "+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(\"itemName\", \"username\") # df with products and username\n",
    "df1 = df1.selectExpr(\"itemName as product\", \"username as username\") # rename col\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[688] at RDD at PythonRDD.scala:53\n",
      "1580\n",
      "[('Kindle Paperwhite', 'Cristina M'), ('Kindle Paperwhite', 'Ricky'), ('Kindle Paperwhite', 'Tedd Gardiner')]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = df1.rdd.map(lambda x: tuple(x))\n",
    "print(rdd1)\n",
    "print(rdd1.count())\n",
    "print(rdd1.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tUsing methods from PySpark’s RDD object e.g. groupByKey, map, reduceByKey derive the top 20 products. \n",
    "\n",
    "Sample: \n",
    "\n",
    "[(u'Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker', 542), \n",
    " (u'Amazon Fire TV', 166), \n",
    " (u'Amazon Premium Headphones', 133), \n",
    " (u'Fire HD 6 Tablet', 87), \n",
    " (u'Kindle Fire HDX 7', 53)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker', 542), ('Amazon Fire TV', 166), ('Amazon Premium Headphones', 133), ('Fire HD 6 Tablet', 87), ('\"Kindle Fire HDX 7\"\"\"', 53), ('\"Kindle Fire HDX 8.9\"\"\"', 43), ('\"Kindle Fire HD 7\"\"\"', 41), ('Kindle Paperwhite', 39), ('Certified Refurbished Amazon Fire TV (Previous Generation - 1st)', 38), ('Kindle Keyboard', 32), ('All-New Amazon Fire 7 Tablet Case (7th Generation', 27), ('Kindle', 20), ('Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders', 19), ('All-New Amazon Fire HD 8 Tablet Case (7th Generation', 18), ('Replacement Remote for Amazon Fire TV Stick', 17), ('All-New Amazon Kid-Proof Case for Amazon Fire 7 Tablet (7th Generation', 13), ('Echo Dot (2nd Generation) - Black', 13), ('All-New Amazon Kid-Proof Case for Amazon Fire HD 8 Tablet (7th Generation', 12), ('Moshi Anti-Glare No Bubble Screen Protector for the Fire Phone', 12), ('Amazon Kindle Oasis Premium Leather Battery Cover - Walnut', 12)]\n"
     ]
    }
   ],
   "source": [
    "def countCust(x):\n",
    "    return len(x)\n",
    "        \n",
    "rdd1_grouped = rdd1.groupByKey()\n",
    "\n",
    "rdd1_key_counts = rdd1_grouped.map(\n",
    "    lambda x: (\n",
    "        x[0], countCust(x[1])\n",
    "    )\n",
    ")\n",
    "\n",
    "best_products_list = tuple(rdd1_key_counts.takeOrdered(rdd1_key_counts.count(), key = lambda x: -x[1]))\n",
    "query_best_prodcuts = sc.parallelize(best_products_list)\n",
    "print(query_best_prodcuts.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tCreate another RDD of tuples from the dataframe with the columns ‘username’ and ‘product’ in that order.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|          username|          product|\n",
      "+------------------+-----------------+\n",
      "|        Cristina M|Kindle Paperwhite|\n",
      "|             Ricky|Kindle Paperwhite|\n",
      "|     Tedd Gardiner|Kindle Paperwhite|\n",
      "|            Dougal|Kindle Paperwhite|\n",
      "|Miljan David Tanic|Kindle Paperwhite|\n",
      "+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(\"username\", \"itemName\") # df with products and username\n",
    "df1 = df1.selectExpr(\"username as username\", \"itemName as product\") # rename col\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[845] at RDD at PythonRDD.scala:53\n",
      "1580\n",
      "[('Cristina M', 'Kindle Paperwhite'), ('Ricky', 'Kindle Paperwhite'), ('Tedd Gardiner', 'Kindle Paperwhite')]\n"
     ]
    }
   ],
   "source": [
    "rdd2 = df1.rdd.map(lambda x: tuple(x))\n",
    "print(rdd2)\n",
    "print(rdd2.count())\n",
    "print(rdd2.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using methods form PySparks’s RDD object product the top 10 customers who purchased the most items. The top 10 list must show the username and a list of all the items they bought with the number of that item they bought. \n",
    "\n",
    "Sample: \n",
    "\n",
    "[(u'A. Younan', ({u'Amazon Premium Headphones': 59}, 59)), \n",
    " (u'William Hardin',   ({u'Amazon Fire TV': 16,     u'Certified Refurbished Amazon Fire TV (Previous Generation - 1st)': 12,     u'Fire HD 6 Tablet': 30}, \n",
    "   58)), \n",
    " (u'Andrew', ({u'Amazon Premium Headphones': 43}, 43))] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\"Things I Love Like', ['\"Kindle Fire HDX 8.9\"\"\"', '\"Kindle Fire HDX 8.9\"\"\"', '\"Kindle Fire HDX 8.9\"\"\"', '\"Kindle Fire HDX 8.9\"\"\"', '\"Kindle Fire HDX 8.9\"\"\"', '\"Kindle Fire HDX 8.9\"\"\"', '\"Kindle Fire HDX 8.9\"\"\"', '\"Kindle Fire HDX 8.9\"\"\"', '\"Kindle Fire HDX 8.9\"\"\"']), ('1-Apr', ['Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker']), ('1215', ['Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker']), ('1234', ['Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker']), ('1soni', ['Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker'])]\n"
     ]
    }
   ],
   "source": [
    "# rdd2_reduced = rdd2.reduceByKey(lambda x, y: x+y)\n",
    "# print(rdd2_reduced.take(3))\n",
    "\n",
    "rdd2_grouped = rdd2.groupByKey()\n",
    "# print(rdd2_grouped.take(3))\n",
    "\n",
    "rdd2_products = rdd2_grouped.map(\n",
    "    lambda x: (\n",
    "        x[0], list(x[1])\n",
    "    )\n",
    ")\n",
    "\n",
    "rdd2_products = rdd2_products.sortByKey()\n",
    "\n",
    "print(rdd2_products.take(5))\n",
    "\n",
    "# def getHashMap(x):\n",
    "\n",
    "# def getCount(x):\n",
    "#     return len(x)\n",
    "\n",
    "# rdd2_mapped = rdd2_grouped.map(\n",
    "#     lambda x: (\n",
    "#         x[0], getHashMap(x), getCount(x[1])\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# print(rdd2_mapped.take(5))\n",
    "\n",
    "\n",
    "\n",
    "# counts = rdd1.countByKey()\n",
    "# # counts = tuple(counts)\n",
    "# rdd1_reduced = sc.parallelize(counts)\n",
    "# # print(counts)\n",
    "# print(rdd1_reduced.take(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
