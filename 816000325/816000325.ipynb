{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID : 816000325\n",
    "## Name: Ajay Sieunarine\n",
    "## Email: ajay.sieunarine@my.uwi.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert the following sentence to a python tuple list of letters and their frequency as they appear. Ignore all non-alpha numeric characters.\n",
    "\n",
    "Sentence: \n",
    "\n",
    "“The quick brown fox jumps over the lazy dog and the fox was very happy”\n",
    "\n",
    "Tuple List:\n",
    "\n",
    "[('h', 1), ('e', 1), ('t', 1), ('q', 1), ('i', 1), ('c', 1), ('u', 1), ('k', 1), ('r', 1), ('b', 1), ('o', 1), ('w', 1), ('n', 1), ('x', 1), ('o', 1), ('f', 1), ('u', 1), ('s', 1), ('j', 1), ('m', 1), ('p', 1), ('r', 1), ('e', 1), ('o', 1), ('v', 1), ('h', 1), ('e', 1), ('t', 1), ('a', 1), ('y', 1), ('z', 1), ('l', 1), ('o', 1), ('d', 1), ('g', 1), ('a', 1), ('d', 1), ('n', 1), ('h', 1), ('e', 1), ('t', 1), ('x', 1), ('o', 1), ('f',1), ('a', 1), ('s', 1), ('w', 1), ('y', 1), ('r', 1), ('e', 1), ('v', 1), ('a', 1), ('h', 1), ('y', 1), ('p', 2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('t', 1), ('h', 1), ('e', 1), ('q', 1), ('u', 1), ('i', 1), ('c', 1), ('k', 1), ('b', 1), ('r', 1), ('o', 1), ('w', 1), ('n', 1), ('f', 1), ('o', 1), ('x', 1), ('j', 1), ('u', 1), ('m', 1), ('p', 1), ('s', 1), ('o', 1), ('v', 1), ('e', 1), ('r', 1), ('t', 1), ('h', 1), ('e', 1), ('l', 1), ('a', 1), ('z', 1), ('y', 1), ('d', 1), ('o', 1), ('g', 1), ('a', 1), ('n', 1), ('d', 1), ('t', 1), ('h', 1), ('e', 1), ('f', 1), ('o', 1), ('x', 1), ('w', 1), ('a', 1), ('s', 1), ('v', 1), ('e', 1), ('r', 1), ('y', 1), ('h', 1), ('a', 1), ('p', 2), ('y', 1)]\n",
      "<class 'list'>\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "foxy_sentence = \"The quick brown fox jumps over the lazy dog and the fox was very happy\"\n",
    "new_sentence = \"\" # empty string which will represent the sentence with all common and letters only (with spaces)\n",
    "\n",
    "for i in foxy_sentence: # populate new sentence\n",
    "    if i.isalpha() or i == \" \":\n",
    "        new_sentence += i.lower()\n",
    "\n",
    "# print (new_sentence)\n",
    "words = new_sentence.split() # words will be an array of strings representing all the words in the new_sentence\n",
    "# print(words)\n",
    "\n",
    "foxy_tuple = [] # the tuple to hold the date\n",
    "\n",
    "# foreach word, grab the frequency of each letter, \n",
    "# and store it in a new tuple, which will then be appended to the actual tuple\n",
    "for word in words: \n",
    "    freqs = {}\n",
    "    for ch in word: # \n",
    "        if ch in freqs:\n",
    "            freqs[ch] += 1\n",
    "        else:\n",
    "            freqs[ch] = 1\n",
    "    temp_tuple = tuple(freqs.items())\n",
    "    foxy_tuple += temp_tuple\n",
    "\n",
    "\n",
    "print(type(foxy_tuple))\n",
    "print(len(foxy_tuple))\n",
    "print(foxy_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a PySpark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported pyspark\n",
      "imported all pyspark libs\n",
      "initialized spark session\n",
      "<SparkContext master=local appName=Assignment 2>\n"
     ]
    }
   ],
   "source": [
    "import pyspark # library for the spark session -> spark will allow us to manipualte RDDs\n",
    "print (\"imported pyspark\")\n",
    "from pyspark.sql import SparkSession # spark session -> allow for dataframe and sql functionality\n",
    "from pyspark.sql import Row # allow us to represent rows from dataframes\n",
    "from pyspark.sql.types import * # get/set datatypes of cols\n",
    "from pyspark.sql.functions import * # epic built-in functions \n",
    "from pyspark.ml.linalg import DenseVector # for linear algebra, uses numpy arrays,\n",
    "from pyspark.ml.feature import StandardScaler # standardize features by removing the mean, and scaling to unit variance\n",
    "print(\"imported all pyspark libs\")\n",
    "import pprint\n",
    "\n",
    "# init spark sesh\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Assignment 2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print (\"initialized spark session\")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert the list of tuples to a PySpark RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('t', 1)\n",
      "('h', 1)\n",
      "('e', 1)\n",
      "('q', 1)\n",
      "('u', 1)\n",
      "('i', 1)\n",
      "('c', 1)\n",
      "('k', 1)\n",
      "('b', 1)\n",
      "('r', 1)\n",
      "('o', 1)\n",
      "('w', 1)\n",
      "('n', 1)\n",
      "('f', 1)\n",
      "('o', 1)\n",
      "('x', 1)\n",
      "('j', 1)\n",
      "('u', 1)\n",
      "('m', 1)\n",
      "('p', 1)\n",
      "('s', 1)\n",
      "('o', 1)\n",
      "('v', 1)\n",
      "('e', 1)\n",
      "('r', 1)\n",
      "('t', 1)\n",
      "('h', 1)\n",
      "('e', 1)\n",
      "('l', 1)\n",
      "('a', 1)\n",
      "('z', 1)\n",
      "('y', 1)\n",
      "('d', 1)\n",
      "('o', 1)\n",
      "('g', 1)\n",
      "('a', 1)\n",
      "('n', 1)\n",
      "('d', 1)\n",
      "('t', 1)\n",
      "('h', 1)\n",
      "('e', 1)\n",
      "('f', 1)\n",
      "('o', 1)\n",
      "('x', 1)\n",
      "('w', 1)\n",
      "('a', 1)\n",
      "('s', 1)\n",
      "('v', 1)\n",
      "('e', 1)\n",
      "('r', 1)\n",
      "('y', 1)\n",
      "('h', 1)\n",
      "('a', 1)\n",
      "('p', 2)\n",
      "('y', 1)\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(foxy_tuple) # make rdd from tuple\n",
    "type(rdd)\n",
    "rdd_list = rdd.collect()\n",
    "for el in rdd_list:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using the methods of PySpark RDD display the letter count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 4)\n",
      "('b', 1)\n",
      "('c', 1)\n",
      "('d', 2)\n",
      "('e', 5)\n",
      "('f', 2)\n",
      "('g', 1)\n",
      "('h', 4)\n",
      "('i', 1)\n",
      "('j', 1)\n",
      "('k', 1)\n",
      "('l', 1)\n",
      "('m', 1)\n",
      "('n', 2)\n",
      "('o', 5)\n",
      "('p', 3)\n",
      "('q', 1)\n",
      "('r', 3)\n",
      "('s', 2)\n",
      "('t', 3)\n",
      "('u', 2)\n",
      "('v', 2)\n",
      "('w', 2)\n",
      "('x', 2)\n",
      "('y', 3)\n",
      "('z', 1)\n"
     ]
    }
   ],
   "source": [
    "rdd_grouped = rdd.reduceByKey(lambda x, y: x+y)\n",
    "rdd_grouped = rdd_grouped.sortByKey()\n",
    "rdd_letter_count = rdd_grouped.collect()\n",
    "for el in rdd_letter_count:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using the methods of PySpark RDD display the letter and number of times they appear in each word in the sentence.\n",
    "\n",
    "Sample:\n",
    "\n",
    "[('a', [1, 1, 1, 1]), ('e', [1, 1, 1, 1, 1]), ('i', [1]), ('m', [1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "('a', [1, 1, 1, 1])\n",
      "('b', [1])\n",
      "('c', [1])\n",
      "('d', [1, 1])\n",
      "('e', [1, 1, 1, 1, 1])\n",
      "('f', [1, 1])\n",
      "('g', [1])\n",
      "('h', [1, 1, 1, 1])\n",
      "('i', [1])\n",
      "('j', [1])\n",
      "('k', [1])\n",
      "('l', [1])\n",
      "('m', [1])\n",
      "('n', [1, 1])\n",
      "('o', [1, 1, 1, 1, 1])\n",
      "('p', [1, 2])\n",
      "('q', [1])\n",
      "('r', [1, 1, 1])\n",
      "('s', [1, 1])\n",
      "('t', [1, 1, 1])\n",
      "('u', [1, 1])\n",
      "('v', [1, 1])\n",
      "('w', [1, 1])\n",
      "('x', [1, 1])\n",
      "('y', [1, 1, 1])\n",
      "('z', [1])\n"
     ]
    }
   ],
   "source": [
    "rdd_group_by_key = rdd.groupByKey() # make each entry in the rdd iterable\n",
    "print(type(rdd))\n",
    "    \n",
    "rdd_word_occurences = rdd_group_by_key.map(\n",
    "    lambda x: (\n",
    "        x[0], # the letter\n",
    "        list(x[1]) # the occurences as a list\n",
    "    )\n",
    ")\n",
    "rdd_word_occurences = rdd_word_occurences.sortByKey() # sort in alphabetical order\n",
    "rdd_list = rdd_word_occurences.collect()\n",
    "for el in rdd_list:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are a frequent user of Amazon.com, you are probably familiar with the lists of related products (books, CDs, etc.) the site features to help customers find what they are looking for. Amazon.com presents several such lists on every page, including “Frequently Bought Together” and “Customers Who Bought This Item Also Bought.” These features have roots and solutions in recommendation engines and systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine:\n",
    "•\tCustomers who bought this item also bought  \n",
    "•\tMost popular items \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tCreate a sql context from PySpark SQLContext.  \t \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x0000026F2C0196C8>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(sc)\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tLoad the Amazon Review Dataset into a PySpark RDD, ensure that each row is properly separated and the headers are matched to their respective columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_reviews.csv MapPartitionsRDD[1987] at textFile at <unknown>:0\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile('amazon_reviews.csv')\n",
    "rdd.collect()\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',id,name,username', '0,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Cristina M', '1,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Ricky', '2,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Tedd Gardiner', '3,AVpe7AsMilAPnD_xQ78G,Kindle Paperwhite,Dougal']\n"
     ]
    }
   ],
   "source": [
    "print(rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda line: line.split(\",\")) # split by delim=','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'id', 'name', 'username'],\n",
       " ['0', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Cristina M'],\n",
       " ['1', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Ricky'],\n",
       " ['2', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Tedd Gardiner'],\n",
       " ['3', 'AVpe7AsMilAPnD_xQ78G', 'Kindle Paperwhite', 'Dougal']]"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5) # if getting split not in list error, run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning some string records by removing non-alphabetical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleanName(x):\n",
    "#     newName = \"\"\n",
    "#     for name in x:\n",
    "#         for ch in name:\n",
    "#             if(ch.isalpha() or ch.isdigit() or ch == ' '):\n",
    "#                 newName += ch\n",
    "#     return newName\n",
    "                \n",
    "\n",
    "# rdd_cleaned = rdd.map(\n",
    "#     lambda x: (\n",
    "#         x[0], x[1], cleanName(x[2]), cleanName(x[3])\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# rdd = rdd_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tConvert the rdd to a PySpark DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping rdd into df...\n",
      "done.\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"mapping rdd into df...\")\n",
    "df = rdd.map(lambda line: Row(\n",
    "    key=line[0],\n",
    "    itemID=line[1],\n",
    "    itemName=line[2],\n",
    "    username=line[3],\n",
    ")).toDF()\n",
    "df = df[['key', 'itemID', 'itemName', 'username']]\n",
    "# df.drop([0,1,2])\n",
    "df = df.where(\"itemName!='name'\") # remove header row\n",
    "print(\"done.\")\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------------+-------------+\n",
      "|key|              itemID|         itemName|     username|\n",
      "+---+--------------------+-----------------+-------------+\n",
      "|  0|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|   Cristina M|\n",
      "|  1|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|        Ricky|\n",
      "|  2|AVpe7AsMilAPnD_xQ78G|Kindle Paperwhite|Tedd Gardiner|\n",
      "+---+--------------------+-----------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- itemID: string (nullable = true)\n",
      " |-- itemName: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tUsing the dataframe from the above question show the top 20 bought products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+-----+\n",
      "|            itemName|count|\n",
      "+--------------------+-----+\n",
      "|Amazon Tap - Alex...|  542|\n",
      "|      Amazon Fire TV|  166|\n",
      "|Amazon Premium He...|  133|\n",
      "|    Fire HD 6 Tablet|   87|\n",
      "|\"Kindle Fire HDX ...|   53|\n",
      "|\"Kindle Fire HDX ...|   43|\n",
      "|\"Kindle Fire HD 7\"\"\"|   41|\n",
      "|   Kindle Paperwhite|   39|\n",
      "|Certified Refurbi...|   38|\n",
      "|     Kindle Keyboard|   32|\n",
      "|All-New Amazon Fi...|   27|\n",
      "|              Kindle|   20|\n",
      "|Amazon 5W USB Off...|   19|\n",
      "|All-New Amazon Fi...|   18|\n",
      "|Replacement Remot...|   17|\n",
      "|Echo Dot (2nd Gen...|   13|\n",
      "|All-New Amazon Ki...|   13|\n",
      "|Moshi Anti-Glare ...|   12|\n",
      "|All-New Amazon Ki...|   12|\n",
      "|Alexa Voice Remot...|   12|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mostBought = df.groupBy(\"itemName\").count()\n",
    "print(type(mostBought))\n",
    "mostBought.sort(\"count\", ascending=False).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tUsing the datafrme from the previous question show the top 20 users and the products that they purchased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+\n",
      "|           username|            itemName|count|\n",
      "+-------------------+--------------------+-----+\n",
      "|          A. Younan|Amazon Premium He...|   59|\n",
      "|             Andrew|Amazon Premium He...|   43|\n",
      "|          Victor L.|Amazon Premium He...|   30|\n",
      "|     William Hardin|    Fire HD 6 Tablet|   30|\n",
      "|            Mike W.|    Fire HD 6 Tablet|   29|\n",
      "|      Earthling1984|    Fire HD 6 Tablet|   28|\n",
      "|     William Hardin|      Amazon Fire TV|   16|\n",
      "|         B. Tarbuck|\"Kindle Fire HDX ...|   16|\n",
      "|              Mandy|      Amazon Fire TV|   16|\n",
      "|                 NF|\"Kindle Fire HDX ...|   15|\n",
      "|                 NF|\"Kindle Fire HD 7\"\"\"|   14|\n",
      "|    Amazon Reviewer|\"Kindle Fire HDX ...|   14|\n",
      "|    Amazon Reviewer|\"Kindle Fire HDX ...|   13|\n",
      "|             Dallas|      Amazon Fire TV|   12|\n",
      "|     William Hardin|Certified Refurbi...|   12|\n",
      "|  Michael Gallagher|\"Kindle Fire HDX ...|   12|\n",
      "|   Gregory P. Baker|      Amazon Fire TV|   11|\n",
      "|              MarkM|\"Kindle Fire HD 7\"\"\"|   10|\n",
      "|              Mandy|Certified Refurbi...|   10|\n",
      "|\"Things I Love Like|\"Kindle Fire HDX ...|    9|\n",
      "+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestCustomers = df.groupBy(\"username\", \"itemName\").count()\n",
    "bestCustomers = bestCustomers.sort(\"count\", ascending=False)\n",
    "bestCustomers.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tCreate a RDD of tuples from the dataframe with only 2 columns ‘product’ and ‘username’ in that order.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|          product|          username|\n",
      "+-----------------+------------------+\n",
      "|Kindle Paperwhite|        Cristina M|\n",
      "|Kindle Paperwhite|             Ricky|\n",
      "|Kindle Paperwhite|     Tedd Gardiner|\n",
      "|Kindle Paperwhite|            Dougal|\n",
      "|Kindle Paperwhite|Miljan David Tanic|\n",
      "+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(\"itemName\", \"username\") # df with products and username\n",
    "df1 = df1.selectExpr(\"itemName as product\", \"username as username\") # rename col\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[2022] at RDD at PythonRDD.scala:53\n",
      "1580\n",
      "[('Kindle Paperwhite', 'Cristina M'), ('Kindle Paperwhite', 'Ricky'), ('Kindle Paperwhite', 'Tedd Gardiner')]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = df1.rdd.map(lambda x: tuple(x))\n",
    "print(rdd1)\n",
    "print(rdd1.count())\n",
    "print(rdd1.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tUsing methods from PySpark’s RDD object e.g. groupByKey, map, reduceByKey derive the top 20 products. \n",
    "\n",
    "Sample: \n",
    "\n",
    "[(u'Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker', 542), \n",
    " (u'Amazon Fire TV', 166), \n",
    " (u'Amazon Premium Headphones', 133), \n",
    " (u'Fire HD 6 Tablet', 87), \n",
    " (u'Kindle Fire HDX 7', 53)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker', 542) \n",
      "\n",
      "('Amazon Fire TV', 166) \n",
      "\n",
      "('Amazon Premium Headphones', 133) \n",
      "\n",
      "('Fire HD 6 Tablet', 87) \n",
      "\n",
      "('\"Kindle Fire HDX 7\"\"\"', 53) \n",
      "\n",
      "('\"Kindle Fire HDX 8.9\"\"\"', 43) \n",
      "\n",
      "('\"Kindle Fire HD 7\"\"\"', 41) \n",
      "\n",
      "('Kindle Paperwhite', 39) \n",
      "\n",
      "('Certified Refurbished Amazon Fire TV (Previous Generation - 1st)', 38) \n",
      "\n",
      "('Kindle Keyboard', 32) \n",
      "\n",
      "('All-New Amazon Fire 7 Tablet Case (7th Generation', 27) \n",
      "\n",
      "('Kindle', 20) \n",
      "\n",
      "('Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders', 19) \n",
      "\n",
      "('All-New Amazon Fire HD 8 Tablet Case (7th Generation', 18) \n",
      "\n",
      "('Replacement Remote for Amazon Fire TV Stick', 17) \n",
      "\n",
      "('All-New Amazon Kid-Proof Case for Amazon Fire 7 Tablet (7th Generation', 13) \n",
      "\n",
      "('Echo Dot (2nd Generation) - Black', 13) \n",
      "\n",
      "('All-New Amazon Kid-Proof Case for Amazon Fire HD 8 Tablet (7th Generation', 12) \n",
      "\n",
      "('Moshi Anti-Glare No Bubble Screen Protector for the Fire Phone', 12) \n",
      "\n",
      "('Amazon Kindle Oasis Premium Leather Battery Cover - Walnut', 12) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def countCust(x):\n",
    "    return len(x)\n",
    "        \n",
    "rdd1_grouped = rdd1.groupByKey()\n",
    "\n",
    "rdd1_key_counts = rdd1_grouped.map(\n",
    "    lambda x: (\n",
    "        x[0], countCust(x[1])\n",
    "    )\n",
    ")\n",
    "\n",
    "best_products_list = tuple(rdd1_key_counts.takeOrdered(rdd1_key_counts.count(), key = lambda x: -x[1]))\n",
    "query_best_prodcuts = sc.parallelize(best_products_list)\n",
    "products_list = query_best_prodcuts.take(20)\n",
    "for el in products_list:\n",
    "    print(el, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tCreate another RDD of tuples from the dataframe with the columns ‘username’ and ‘product’ in that order.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|          username|          product|\n",
      "+------------------+-----------------+\n",
      "|        Cristina M|Kindle Paperwhite|\n",
      "|             Ricky|Kindle Paperwhite|\n",
      "|     Tedd Gardiner|Kindle Paperwhite|\n",
      "|            Dougal|Kindle Paperwhite|\n",
      "|Miljan David Tanic|Kindle Paperwhite|\n",
      "+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(\"username\", \"itemName\") # df with products and username\n",
    "df1 = df1.selectExpr(\"username as username\", \"itemName as product\") # rename col\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[2042] at RDD at PythonRDD.scala:53\n",
      "1580\n",
      "[('Cristina M', 'Kindle Paperwhite'), ('Ricky', 'Kindle Paperwhite'), ('Tedd Gardiner', 'Kindle Paperwhite')]\n"
     ]
    }
   ],
   "source": [
    "rdd2 = df1.rdd.map(lambda x: tuple(x))\n",
    "print(rdd2)\n",
    "print(rdd2.count())\n",
    "print(rdd2.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using methods form PySparks’s RDD object product the top 10 customers who purchased the most items. The top 10 list must show the username and a list of all the items they bought with the number of that item they bought. \n",
    "\n",
    "Sample: \n",
    "\n",
    "[(u'A. Younan', ({u'Amazon Premium Headphones': 59}, 59)), \n",
    " (u'William Hardin',   ({u'Amazon Fire TV': 16,     u'Certified Refurbished Amazon Fire TV (Previous Generation - 1st)': 12,     u'Fire HD 6 Tablet': 30}, \n",
    "   58)), \n",
    " (u'Andrew', ({u'Amazon Premium Headphones': 43}, 43))] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "('A. Younan', {'Amazon Premium Headphones': 59}, 59) \n",
      "\n",
      "('William Hardin', {'Certified Refurbished Amazon Fire TV (Previous Generation - 1st)': 12, 'Amazon Fire TV': 16, 'Fire HD 6 Tablet': 30}, 58) \n",
      "\n",
      "('Andrew', {'Amazon Premium Headphones': 43}, 43) \n",
      "\n",
      "('Victor L.', {'Amazon Premium Headphones': 30}, 30) \n",
      "\n",
      "('Mike W.', {'Fire HD 6 Tablet': 29}, 29) \n",
      "\n",
      "('NF', {'\"Kindle Fire HD 7\"\"\"': 14, '\"Kindle Fire HDX 7\"\"\"': 15}, 29) \n",
      "\n",
      "('Earthling1984', {'Fire HD 6 Tablet': 28}, 28) \n",
      "\n",
      "('Amazon Reviewer', {'\"Kindle Fire HDX 8.9\"\"\"': 14, '\"Kindle Fire HDX 7\"\"\"': 13}, 27) \n",
      "\n",
      "('Mandy', {'Certified Refurbished Amazon Fire TV (Previous Generation - 1st)': 10, 'Amazon Fire TV': 16}, 26) \n",
      "\n",
      "('Amazon Customer', {'\"Kindle Fire HDX 8.9\"\"\"': 1, 'Certified Refurbished Fire HD 10 Tablet': 1, 'Certified Refurbished Kindle E-reader - Black': 1, 'Certified Refurbished Echo Dot (2nd Generation) - Black': 1, 'Amazon Fire TV': 3, 'Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders': 1, 'All-New Amazon Fire 7 Tablet Case (7th Generation': 2, 'All-New Amazon Kid-Proof Case for Amazon Fire 7 Tablet (7th Generation': 2, 'All-New Fire HD 8 Tablet with Alexa': 1, 'All-New Fire HD 8 Kids Edition Tablet': 1, 'Kindle for Kids Bundle with the latest Kindle E-reader': 1, 'All-New Fire 7 Kids Edition Tablet': 1, '\"Kindle Fire HD 7\"\"\"': 1, 'Moshi Anti-Glare No Bubble Screen Protector for the Fire Phone': 1, 'Replacement Remote for Amazon Fire TV Stick': 1, 'Alexa Voice Remote for Amazon Fire TV and Fire TV Stick': 3}, 22) \n",
      "\n",
      "('Dallas', {'Certified Refurbished Amazon Fire TV (Previous Generation - 1st)': 8, 'Amazon Fire TV': 12}, 20) \n",
      "\n",
      "('Gregory P. Baker', {'Certified Refurbished Amazon Fire TV (Previous Generation - 1st)': 8, 'Amazon Fire TV': 11}, 19) \n",
      "\n",
      "('Michael Gallagher', {'Fire HD 7 Tablet': 1, 'Certified Refurbished Kindle E-reader': 2, 'Fire HDX 8.9 Tablet': 1, 'Kindle': 1, '\"Kindle Fire HDX 7\"\"\"': 12}, 17) \n",
      "\n",
      "('B. Tarbuck', {'\"Kindle Fire HDX 8.9\"\"\"': 16}, 16) \n",
      "\n",
      "('J. Chambers', {'Kindle Paperwhite E-reader - Black': 2, 'Kindle Paperwhite 3G': 1, 'Kindle Voyage E-reader': 1, 'Certified Refurbished Kindle Paperwhite E-reader - Black': 1, 'Kindle Oasis E-reader with Leather Charging Cover - Walnut': 1, 'Certified Refurbished Kindle Voyage E-reader with Special Offers': 1, 'All-New Amazon Fire HD 8 Tablet Case (7th Generation': 1, 'Kindle Paperwhite': 5}, 13) \n",
      "\n",
      "('D. Miyao', {'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Indigo Fabric': 2, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Saddle Tan Leather': 2, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Sandstone Fabric': 1, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Merlot Leather': 1, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Midnight Leather': 2, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Charcoal Fabric': 2}, 10) \n",
      "\n",
      "('MarkM', {'\"Kindle Fire HD 7\"\"\"': 10}, 10) \n",
      "\n",
      "('\"Things I Love Like', {'\"Kindle Fire HDX 8.9\"\"\"': 9}, 9) \n",
      "\n",
      "('Judy Schechter', {'\"Kindle Fire HD 7\"\"\"': 8}, 8) \n",
      "\n",
      "('Bob', {'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Indigo Fabric': 1, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Saddle Tan Leather': 1, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Sandstone Fabric': 1, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Merlot Leather': 1, 'Amazon Fire TV': 1, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Midnight Leather': 1, 'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Charcoal Fabric': 1}, 7) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd2_grouped = rdd2.groupByKey()\n",
    "# print(rdd2_grouped.take(3))\n",
    "\n",
    "rdd2_products = rdd2_grouped.map(\n",
    "    lambda x: (\n",
    "        x[0], list(x[1])\n",
    "    )\n",
    ")\n",
    "\n",
    "rdd2_products = rdd2_products.sortByKey()\n",
    "\n",
    "# print(rdd2_products.take(2))\n",
    "\n",
    "def getDict(x):\n",
    "    freqs = {}\n",
    "    for item in x:\n",
    "        if item in freqs:\n",
    "            freqs[item] += 1\n",
    "        else:\n",
    "            freqs[item] = 1\n",
    "    return freqs\n",
    "\n",
    "rdd2_products_count = rdd2_products.map(\n",
    "    lambda x: (\n",
    "        x[0], getDict(x[1])\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(rdd2_products_count.take(2))\n",
    "\n",
    "def getTotal(x):\n",
    "    total = 0\n",
    "    index = 0\n",
    "    for item in x:\n",
    "        total += x[item]\n",
    "        index += 1\n",
    "    return total\n",
    "\n",
    "rdd2_products_count_total = rdd2_products_count.map(\n",
    "    lambda x: (\n",
    "        x[0], x[1], getTotal(x[1])\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(rdd2_products_count_total.take(5))\n",
    "\n",
    "print(\"\\n\")\n",
    "best_cust_list = tuple(rdd2_products_count_total.takeOrdered(rdd2_products_count_total.count(), key = lambda x: -x[2]))\n",
    "query_best_cust = sc.parallelize(best_cust_list)\n",
    "# pprint(query_best_cust.take(10))\n",
    "cust_list = query_best_cust.take(20)\n",
    "for el in cust_list:\n",
    "    print(el , \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
